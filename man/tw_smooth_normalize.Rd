% Generated by roxygen2 (4.1.1): do not edit by hand
% Please edit documentation in R/matrices.R
\name{tw_smooth_normalize}
\alias{tw_blei_lafferty}
\alias{tw_sievert_shirley}
\alias{tw_smooth}
\alias{tw_smooth_normalize}
\title{Scoring methods for words in topics}
\usage{
tw_smooth_normalize(m)

tw_smooth(m)

tw_blei_lafferty(m)

tw_sievert_shirley(m, lambda = 0.6)
}
\arguments{
\item{m}{a \code{\link{mallet_model}} object}

\item{lambda}{For \code{sievert_shirley}, the weighting parameter
\eqn{\lambda}, by default 0.6.}
}
\value{
a function of one variable, to be applied to the topic-word sparse
  matrix.
}
\description{
The "raw" final sampling state of words in topics may be transformed into
either estimated probabilities or other kinds of salience scores. These
methods produce \emph{functions} that operate on a topic-word matrix. They
can be passed as the \code{weighting} parameter to \code{\link{top_words}}.
}
\details{
The basic method (\code{tw_smooth_normalize}) is to recast the sampled word
counts as probabilities by adding the estimated hyperparameter \eqn{\beta}
and then normalizing rows so they add to 1. This is equivalent to
\code{\link[mallet]{mallet.topic.words}} with \code{smooth} and
\code{normalize} set to TRUE. Naturally this will not change the relative
ordering of words within topics.

\code{tw_smooth} simply adds \eqn{\beta} but does not normalize.

A method that can re-rank words has been given by Blei and Lafferty: the
score for word \eqn{v} in topic \eqn{t} is \deqn{p(t,v) log(p(t,v) /
\prod_k p(k,v)^(1/K))} where \eqn{K} is the number of topics. The score gives
more weight to words which are ranked highly in fewer topics.

Another method is the "relevance" score of Sievert and Shirley: in this case
the score is given by \deqn{\lambda log(p(t,v) + (1 - \lambda) log(p(t,v) /
p(v)} where \eqn{\lambda} is a weighting parameter which is by default set to
0.6 and which determines the amount by which words common in the whole corpus
are penalized.
}
\examples{
\dontrun{top_words(m, n=10, weighting=tw_blei_lafferty(x))}
\dontrun{tw_smooth_normalize(m)(topic_words(m))}
}
\references{
D. Blei and J. Lafferty. Topic Models. In A. Srivastava and M.
  Sahami, editors, \emph{Text Mining: Classification, Clustering, and
  Applications}. Chapman & Hall/CRC Data Mining and Knowledge Discovery
  Series, 2009.
  \url{http://www.cs.princeton.edu/~blei/papers/BleiLafferty2009.pdf}.

  C. Sievert and K.E. Shirley. LDAvis: A method for visualizing and
  interpreting topics.
  \url{http://nlp.stanford.edu/events/illvi2014/papers/sievert-illvi2014.pdf}.
}

